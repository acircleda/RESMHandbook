# Data Preparation and Cleaning in R

```{r package_setup, echo=FALSE, message=FALSE, warning=FALSE}
library(psych)
library(tidyverse)
library(skimr)
```

This chapter will introduce you to viewing your data, summarizing your data, and cleaning your data following recommendations from the [*Brief Introduction to the 12 Steps of Data Cleaning*](https://www.slideshare.net/jamorrow/brief-introduction-to-the-12-steps-of-evaluagio) (Morrow, 2013).

## Introduction to the Tidyverse

The `Tidyverse` is a set of packages that make R easier to use. All the packages work together and share an underlying grammar and philosophy. That's right - philosophy. The `Tidyverse` operates on the assumption that data should be "tidy".

According to Hadley Wickham,Chief Scientist at RStudio and one of the creators of the `Tidyverse`: 

> Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:
> 1. Each variable forms a column.
> 2. Each observation forms a row.
> 3. Each type of observational unit forms a table.

You can read more in [*Tidy Data* from the Journal of Statistic Software](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf).

The `Tidyverse` not only helps keep data "tidy," but makes programming easier compared to the base R syntax. Compare:

### Extracting a variable

**Tidyverse**
```
select(iris, Species, Petal.Width) # by name
select(iris, 5, 4)  # by column index
```
**Base R**
```
iris[, c("Species", "Petal.Width")] # by name
iris[, c(5, 4)]  # by column index
```
### Make new variables (columns)

**Tidyverse**
```
iris %>%
mutate(Petal.Ratio = Petal.Length/Petal.Width,
       Sepal.Ratio = Sepal.Length/Sepal.Width)
```

**Base R**
```
iris$Petal.Ratio <- iris$Petal.Length/iris$Petal.Width
iris$Sepal.Ratio <- iris$Sepal.Length/iris$Sepal.Width
```
(see more examples [here](https://tavareshugo.github.io/data_carpentry_extras/base-r_tidyverse_equivalents/base-r_tidyverse_equivalents.html))

Most of this handbook will use `Tidyverse` functions, as they are clearly easier to work with.

## Viewing Your Data

First, let's load some data. We will be working with [R Studio's "Learning R Survey"](https://github.com/rstudio/learning-r-survey).

We will load the data from the URL:

```{r}
raw_data <- read.csv("https://github.com/rstudio/learning-r-survey/blob/master/2019/data/2019%20English%20R%20Community%20Survey%20Responses%20.csv?raw=true", fileEncoding = "UTF-8")
```

We also added `fileEncoding = "UTF-8"` to make sure any text was formatted correctly. UTF-8 is a common encoding format.

### The RStudio Environment

There are several ways to view this data using the R Studio environment:

1. Type the name in the **console**.
  + This is not useful for a large data frame.
2. Double-click the data object in the **environment**.
  + This will open up the dataframe in the **source** pane, and you can easily browse the data.
  + You can do the same thing using the `view()` function, i.e. `view(raw_data)`.

### Structure of Data - str()

You can use `str()` (from base R) to view how your data is structured. That is, whether variables are numeric, characters, factors. If they are factors, you can also see what their levels are. We recommend using `str()` in the **console** not your script.

This is a very wide dataframe (52 variables/columns), so we will use the following code just to view the structure of the first 5 columns.

```{r}
str(raw_data[1:5])
```

Here is what we did:

* `str(raw_data)` tells R you want to look at the structure of the data object.
* `[1:5]` tells R you just want to look at columns 1 to 5

Here is what the output means:

1. "Timestamp" contains dates but is being read as a **factor**. Factors are like categories. Multiple data points can share a factor.
  + This can be converted into a date later using the `lubridate` package
2. "How.would.you.rate.your.level.of.experience.using.R." is also a factor. It has 5 levels or categories. "Beginner" is one level. The numbers after the list correspond to the levels of the factors. For example, "Beginner" might be 1 or 2 or 3. We will investigate this more later.
3. "Compared.with.other...." has the class **integer** (int). This is a number. The first 10 numbers are all NA (missing).

If you just want to examine a single variable, you can use the **`$`** operator:

```{r}
str(raw_data$In.what.country.do.you.currently.reside.)
```

In the output, we can see:

  + `Factor` - This is a factor (a categorical variable)
  + `w/ 93 levels` - It has 93 categories (countries)
  + `"", "Afghanistan",..:` - The first category is "" (i.e. missing), the second category is Afghanistan...and so on, in alphabetical order
  + `90 57 90 90 90` - These numbers represent the numbers (randomly) assigned to each category. 

If you just want to see the class of a variable, you can use the `class` function:

```{r}
class(raw_data$What.year.did.you.first.start.learning.R.)
```

### Variable/Column Names - names()

The `names()` (from base R) function is useful if you just want a list of the variable names.

```{r}
names(raw_data[1:5])
```

### First n Rows - head()

The `head()` (from base R's `utils` package) function gives you a slightly more detailed look at your data. It will give you the column name and the first 10 data points (by default):

```{r}
head(raw_data[1:5])
```

You can also request more or less data points per column:

```{r}
head(raw_data[1:5], n = 3)
```

Or you can request data from a single variable:

```{r}
head(raw_data$How.likely.are.you.to.recommend.R.to.a.colleague..friend..or.family.member., n = 20)
```

`head()` looks at the head or top of the data. `tail()` looks at the bottom:

```{r}
tail(raw_data$How.likely.are.you.to.recommend.R.to.a.colleague..friend..or.family.member., n = 20)
```

## Renaming Variables

If you have noticed, the variable names for the R survey are very long. This is not convenient to work with.

R Studio provides us with a useful [code book](https://github.com/rstudio/learning-r-survey/blob/master/2019/Learning%20R%20Internet%20Survey%20-%20Question%20Names.tsv) to help us fix this problem.

A common way of renaming columns is to use the `rename()` function. This function is part of one of the most useful packages you will download and install: `tidyverse`.

To begin, install tidyverse through the **console**: `install.packages("tidyverse")`. Then load it at the top of your script:
`library(tidyverse)`. Don't forget to run this line of code!

The first column's name is already simple: *Timestamp*. So, let's rename the second column.

First, let's get the column name (run this in the **console**:

```{r}
names(raw_data[2])
```

Now, we can use the `rename()` function to change this to "Qr_experience", as listed in the codebook. We will assign this to a new data object for comparison purposes. Put this in your script.

```{r}
renamed <- raw_data %>%
  rename("Qr_experience" = "How.would.you.rate.your.level.of.experience.using.R.")

```

Let's see the new name:

```{r}
# new name
names(renamed[1:2])
```

So, what did we do?

* `renamed <-` creates a new data object
* `raw_data` says which data object we are working with
* `%>%` is the pipe operator. This means something like "and"
* `rename("new_name" = "old name")` sets the name name and the old name. Note the quotation marks

We can read this line as a sentence: Create a data object, "renamed". Use the "raw_data" object and rename "new_name" from "old_name".


## Renaming Multiple Variables

We can also rename multiple columns the same way:

```{r}
renamed <- raw_data %>%
  rename("Qr_experience" = "How.would.you.rate.your.level.of.experience.using.R.",
         "Qr_difficulty_experienced" = "Compared.with.other.technical.topics.you.ve.learned.in.school.and.on.the.job..on.a.scale.of.1.to.5..how.difficult.do.you.expect.learning.R.to.be.")

```

Let's check:

```{r}
# new name
names(renamed[2:3])
```

You can also use `names()` to change the names of the data frame like this. You can use `c` to write a list of names and then apply it to the names of your data frame. The names are applied in the order they are written and any misisng names are set to `NA`:

```{r}
names(renamed) <- c("Q1", "Q2", "Q3")
```

Let's check:
```{r}
# new name
names(renamed[1:2])
```



However, raw_data has 52 variables! This would take a *long* time to do.
Instead of writing 52+ lines of code, we can use the `names()` function to set the names of our data frame (raw_data) equal to the names of *another* data frame.

The R Studio survey provides us with a .tsv file of proper variable names for the columns. Let's load that first:

```{r message=FALSE}
qnames <- read_tsv("https://raw.githubusercontent.com/rstudio/learning-r-survey/master/2019/data/2019-english-question-names-only.tsv")
```


Let's use the method above to make a copy of "raw_data" and then rename it:

```{r}
# make a copy of the data frame 
rsurvey <- raw_data

# rename the columns based on qnames
names(rsurvey) <- names(qnames)
```

Let's check

```{r}
names(rsurvey[1:4])
```

Note that renaming follows the order of the "qnames" data frame.

Note also that we named the new data object "rsurvey". This is the beginning of our clean data set. We don't want to change anything in our original "raw_data" data frame.

## Cleaning Names with janitor

Let's make sure all the names are lowercase. This will make typing them in later analyses easier, as you don't ever need to remember what is capital and what is not. Install the `janitor` package:

**Console**: `install.packages("janitor")`.

Run the following code:

```{r}
rsurvey <- rsurvey %>% 
  janitor::clean_names()
```

Let's check:

```{r}
names(rsurvey[1:10])
```

The `clean_names()` function will do a number of things:

* convert to lower case
* change spaces into underscores
* change % signs to "percentage"

There are many cool options, so please check `?janitor::clean_names`.

### Summary Stats - describe()

The `psych` package offers a very useful funcion called `describe()`. This function gives you summary statistics about your data. In particular, it will give you:

* item name 
* item number 
* number of valid cases
* mean
* standard deviation
* trimmed mean (with trim defaulting to .1) 
* median (standard or interpolated
* mad: median absolute deviation (from the median). 
* minimum
* maximum
* skew
* kurtosis
* standard error

Load the `psych` package at the top of your script using `library(psych)`. Then, you can execute this command.

The following command describes the third variable (according to our `str()` function, that variable is numeric):

```{r}
describe(rsurvey[1:5])
```

See more options using `?describe` in the **console**.

### Summary Stats - describeBy()

The `psych` package also provides `describeBy` (note the capital "B") to break an integer variable down by some factor or category. For example, the survey provides "qyear_born", which we can use to calculate age. Then, we can see summary stats of age by experience) from "qr_experience".

```{r}
#create an age variable
rsurvey$age <- 2020-rsurvey$qyear_born

#Describe experience by age
describeBy(rsurvey$age, group=rsurvey$qr_experience, mat=TRUE)
```

What did we do?

  * `rsurvey$age <-` - Assign whatever is after the `<-` to a new column, "age" in "rsurvey"
  * `2020-rsurvey$qyear_born` - Subtract rsurvey$qyear_born from the current year, 2020.
  * `describeBy(age$age,` - The numeric variable
  * `group=age$qr_experience` - The grouping variable
  * `mat=TRUE)` - Show a matrix in the output instead of a more complicated list
  

### Summary Stats - summary()

The `summary()` function will give similar statistics as `describe()`, though with less detail. For **factors**, it will produce a count per level (category). For **integers** it will produce min, max, quartiles, median, mean, and number of missing data.

The following `summary()` function calls columns 2 (a factor) and 3 (an integer)

```{r}
summary(rsurvey[2:3])
```

### Summary Stats - skim()

`skim` is another function that produces summary statistics. It will also produce a small histogram to show the distribution of your data (for numeric/integers).

First, install the `skimr` package in the **console** using:

```install.packages("skimr")```

Add ```library(skimr)``` to the top of your script after installation.

Then, we can use the `skim()` function like this:

```{r}
skim(rsurvey[2:3])
```


### Crosstabs - table()

Base `R` provides the `table()` function to allow you to view a crosstabulation count of one variable by one or more other variables

We can use the code below to get a list of experience levels by country:

```{r}
table(rsurvey$qcountry, rsurvey$qr_experience)[1:5,]
```

**A Note about `[ ]`

>In the example above, [1:5,] was used. This tells R to take the crosstabs table and look at rows 1:5. Otherwise, there would have been nearly 100 rows of data printed. `[]` can be used to refer to a specific rows and columns. It specifies a column index. It is used like this:

  * `data[1]` - gets the first column
  * `data[1:5]` - gets columns 1 to 5
  * `data[1,]` - gets the first row
  * `data[1:5,]` - gets rows 1 to 5
  * `data[1,1]` - gets the first row and first column
  * `data[1:5,1:5]` - gets the first five rows and the first five columns


## Keeping and Dropping Variables

We can use `select()` from `tidyverse`'s `dplyr` package to keep specific variables or drop other variables.

**Keeping** variables:

```{r}
keep <- rsurvey %>% select(qr_experience, qr_year)
```

To **drop** variables, use the *minus* sign.

```{r}
dropped <- rsurvey %>% select(-qr_experience, -qr_year)
```

For multiple variables, you can use commas for single columns and colons for a range of columns:

```{r}
keep_multi <- rsurvey %>%
  select(qr_experience, -qr_year, qr_difficulty:qobstacles_to_starting)

```

Note that `select` also places variables in the order specificed.

Alternatively, you can do the same in base R with `subset`:

```
keep <- subset(rsurvey, select = qr_experience)
dropped <- subset(rsurvey, select = -qr_experience)
```

`select` has more advanced functions such as `select_if` and `select_at`. See `?select` or `?select_if` for more information.

## Keeping and Dropping Rows

Whereas `select()` works on columns, `slice()` works on rows:

  * `slice(1:100)` keeps rows 1 to 100
  * `slice(-1:-100)` removes rows 1 to 100
  * `slice(1)` keeps just row 1
  * `slice(-1)` removes just row 1


## More Frequencies and Descriptives

You can use any of the following functions to get frequencies of your data:

* `summary()`
* `describe()` from the `psych` package
* `skim()` from the `skimr` package

Let's use the `jmv` package the get some frequencies:

**Console**: `install.packages("jmv")`

We will use the `descriptives()` function. This will provide nice output. We will use the parameter `freq=TRUE` to get frequencies for factors.

```{r message=FALSE, warning=FALSE}
library("jmv")

rsurvey %>%
  select(qr_experience, qr_year) %>%
  descriptives(freq = TRUE)
```

## Spotting Coding Mistakes

You may have noticed that the descriptives for "qr_year" had a minimum of 2. The question reads "What year did you first start learning R?". This should only contain year data. By running frequencies, you can spot such errors. Let's take a closer look.

Let's start by viewing all the data in this question. Since it is year data, let's arrange it from smallest to greatest and look at the first 10 observations.

```{r}
rsurvey %>%
  select(qr_year) %>%
  arrange(qr_year) %>%
  head(n = 10)
```

Let's review what we did in the code:

* `rsurvey %>%` tells R to use "rsurvey" and...
* `select(qr_year) %>%` select the "qr_year" variable and...
* `arrange(qr_year) %>%` arrange the data based on the "qr_year" variable and...
* `head(n = 10)` get the first 10 responses.

From the results, we saw that the first 5 responses are invalid years.

## Modifying Data - mutate()

We have a variable, "qr_year" with five invalid cases. They should be years, but are some other numbers. Let's change those to mising observations using `mutate()`. `mutate()` allows us to change and create variables (literally mutate the data).

```{r}
#run this to change the variable
rsurvey <- rsurvey %>%
  mutate(qr_year2 = ifelse(qr_year < 1977, NA, qr_year))

#run this to check
rsurvey %>%
  select(qr_year, qr_year2) %>%
  arrange(qr_year) %>%
  head(n=10)
```

What we did:

* `rsurvey <-` means we will save our fixed data to the "rsurvey" data object. Essentially, we are overwriting the data object that already exists.
* `rsurvey %>%` tells R to use "rsurvey" and...
* `mutate(qr_year2` create a new variable, "qr_year2"
* `=` which is equal to the following command
* `ifelse(qr_year < 1977, NA, qr_year))` if qr_year is less than 1977, write NA, otherwise (`else`), write the year from qr_year

`ifelse` is a conditional statement that says: `ifelse(if this condition is true, then do this, otherwise do this)`.

## Reordering Categories - factor()

Recode allows you to take a numeric value in a dataset and assign it a label, for example make 1 = "apple", or the opposite, for example, make "apple" = 1.

Many of the factors (categories) in our R survey dataset have already been coded to a numeric value, automatically. However, this does not mean the mapping of the number to the factor is correct.

Let's run this code in the **console** to check. Since we are just checking, we don't really want to include this in our script, so we can do basic exploration in the console.


```{r}
rsurvey %>% count(qr_experience)
```

You will see that 1 = "", which is missing data, 2 = "Beginner", 3 = "Expert", 4 = "Intermediate" and 5 = "None". If we were doing a statistical analysis without modifying our data, our results would be severely inaccurate.

Let's recode this data. We will just do one variable as an example.

```{r}
recoded <- rsurvey %>% 
  select(qr_experience) %>%
  mutate(qr_experience2 = factor(qr_experience,
        levels=c("None","Beginner", "Intermediate", "Expert", NA ))
  )
```

Let's check if we are correct. You can run this in the **console**:
```{r}
recoded %>% count(qr_experience2)
```

Now our factors are in the correct order and if we wanted to do some statistical test, it would be much more accurate.

What did we do?

* `recoded <-` creates a new data object. We could have overwritten our previous object, but this is to show you the result of recoding.
* `rsurvey %>%` tells R to use the "rsurvey" data object and...
* `select(qr_experience) %>%` means we are only going to keep this variable in our data (to make demonstration easier), and 
* `mutate(` means we are going to make a change
* `qr_experience2 =` we will create a new variable "qr_experience2" equal to the next command
  + Note: we could have done `mutate(qr_experience)` and overwritten our column, but if we create a new variable, we can compare them side by side (for demonstration purposes)
* `factor(qr_experience,` make a factor from "qr_experience"
* `levels=c("None","Beginner", "Intermediate", "Expert", NA ))` set the level names and orders to these.
  + Note that `NA` refers to missing data and is **not** in quotes
* `)` this final parentheses closes the `mutate` function. If you place your cursor after it, the corresponding opening parentheses will be highlighted. This is an easy way to check that you have closed all functions, useful for debugging problems.
  )
  
This works, so lets apply it to the real data set:

```{r message=FALSE, warning=FALSE}
rsurvey <- rsurvey %>% 
  mutate(qr_experience = factor(qr_experience,
        levels=c("None","Beginner", "Intermediate", "Expert", NA ))
  )
```

## Clearing Whitespace in Text - str_trim()

On the Learning R survey, there was a question about what industry people were in. This resulted in 135 different responses. However, many of these can be combined to make several large groups. First, let's view the jobs. To save space, only the first 10 are shown.

```{r}
rsurvey %>%
  count(qindustry) %>%
  slice(1:20)
```

The `count()` function counts *unique* values. However, you will notice that many are repeated. That is likely due to random whitespace in or around the text entries. We can use several functions `str_trim()` from `tidyverse`'s `stringr` package to fix this.

`stringr` is loaded when the `tidyverse` is loaded, so we don't need to load another library.

`str_trim()` removes whitespace from the start and end of a string. Let's also use make sure everything is lowercase using either base R's `tolower()` or `stringr`'s `str_to_lower()`. They both do the same thing. You can also use `str_squish` to remove **all** white space within a string.

```{r}
rsurvey <- rsurvey %>%
  mutate(qindustry = str_squish(tolower(qindustry)))
```

Let's check. You can type this in the console. We will select only "qindustry" and "qindustry2" to compare them:

```{r}
rsurvey %>%
  count(qindustry) %>%
  slice(1:10)
```

Hmm. That reduced the number of unique values to 125. That is not ideal, but we have a few more tools we can use to fix this.

First, what did we do?

  * `rsurvey <- ` - saves the following commands to the "rsurvey" data object
  * `rsurvey %>%` - means to use the "rsurvey" data object "and then"
  * `mutate(` - change the data
  * `qindustry = ` - make the column "qindustry" equal to the following
  * `str_squish(` - trim the whitespace in the following string
  * `tolower(qindustry)))` - make the string lowercase and close all parentheses
  
**A Note about Parentheses ( )**

> In R, it can be easy to become lost with your open and closing parentheses. RStudio provides parenthesese highlighting. Place your cursor on the *right* side of any parentheses (open or close) and it will highlight to corresponding one. This is a quick way to debug a common problem.

## Combining Categories - case_when()

Looking through the list of jobs, there are some clear categories that we can combine jobs into. For example, *agriculture*, *agriculture and anaimal science*, and *agrifood* can all be combine into an "agriculture" category. Here is what we will do. We will use three functions to do this:
  * `mutate()` to change or create new variables
  * `case_when()` as a kind of if/else command
  * `str_detect()` to select specified text

```{r}
# combine into "agriculture"
rsurvey <- rsurvey %>%
  mutate(
    industry = case_when(
      str_detect(qindustry, "agri") ~ "agriculture",
      TRUE ~ qindustry
    )
  )

# and check total number - it should
rsurvey %>% 
  select(industry) %>%
  distinct() %>% #remove duplicates
  count()
```
We now have 123 rows. Let's see what we did, and then expand it:

  * `rsurvey <-` - assign to the "rsurvey" object
  * `rsurvey %>%` - use "rsurvey" and then
  * `mutate(` - change or create a variable
  * `industry = case_when(` - create the variable "industry" when
  * `str_detect(qindustry, "agri")` - R detects "agri" in the "qindustry" columns"
  * `~ "agriculture",` - and rename them "agriculture"
  * `TRUE ~ qindustry` - everything else is named the name they have in "qindustry"
  
Let's exand this to categorize as much as possible. To do this efficiently, We will run `rsurvey$industry` in the *console* and scroll through it as we create categories

```{r}
# combine into "agriculture"
rsurvey <- rsurvey %>%
  mutate(
    industry = case_when(
      str_detect(qindustry, "agri") ~ "agriculture",
      str_detect(qindustry, "health") ~ "health",
      str_detect(qindustry, "education|academia|university|research") ~ "education and research",
      str_detect(qindustry, "marketing|business|trade|ecommerce") ~ "business",
      str_detect(qindustry, "information|analytics|software|cybersecurity|digital|telec") ~ "information technologies",
      str_detect(qindustry, "envi|forest|geo|natural|wildlife|sustain") ~ "environment",
      str_detect(qindustry, "law|legal") ~ "law",
      str_detect(qindustry, "media|journalism") ~ "media",
      str_detect(qindustry, "profit|") ~ "non-profit",
      TRUE ~ "other"
    )
  )

# and check
rsurvey %>% count(industry)
```

This code got out data down to 9 categories. The categories are neither perfect nor exhaustive, but they are functional. This is a lesson in working with open-ended data on surveys. A question like this should really have limited options, otherwise, there is a lot to clean!

**A Note About Spacing and Tabs**

>You may have noticed that in the code above, `mutate()` was on its own line and there were two parentheses also on their own line. You may have also noticed `mutate()` was indented as was `str_detect()`. These are not required for the code to function, but are useful to keep the code organized.

>It is recommend to start a new function after the %>% pipe operator on a new line. It is also recommended to create a new line after a parentheses( if it will contain multiple or long arguments.

What did we do differently?
  * `"law|legal"` - You will notice we used the | pipe symbol. This stands for "or". This command means to look for "law" or "legal".
  * `"envi|forest|geo|natural|wildlife|sustain"` - You will notice we did not use complete words. `str_detect()` looks for patterns of string and does not need the complete words unless there are many similar string.
    + This means "envi" could look for *environment*, *environments*, or *environmental*.
  * `TRUE ~ "other"` - We set everything else to the "other" category.


## Splitting variables with split()

The Learning R survey contained a few check-all question types. Depending on the survey platform, these questions either create a new column for each choice or combine all choices in a single column. The Learning R survey did the latter. For example, the question "What applications do you use R for most? (check all that apply)" for the variable qused_for looks like this:

```{r}
head(rsurvey$qused_for)
```

How can we work with this type of data? The first thing we can do is split the data so that choices are separated into columns. There are several functions we can use to do this. Normally, we can do this with `separate()` from `tidyverse`'s `tidyr` package. However, we need to know how many columns to split the data into.

We can use the following code to count how many commas appear in each row and then get the highest value:



```{r}
rsurvey$qused_for %>%
  str_count(pattern = ",") %>% 
  max()
```

There is at least one row with 10 commas, meaning 11 items were selected. We can now use separate to separate the string.

```{r}
rsurvey <- rsurvey %>%
  separate(qused_for, sep = ",",
           into = paste0("use_", 1:11))
```

The original data object had 56 variables. The new data object now has 66. This means we have removed the "qused_for" variable and replaced it with 11 more.

What did we do?

  * `rsurvey <-` - assign the new resulting data to "rsurvey"
  * `rsurvey %>%` - use "rsurvey" and then
  * `separate(qused_for, sep = ",",` - separate "qused_for" by using the commas
  * `into = paste0("use_", 1:11))` - into 11 columns, with the name "use_" pasted to each column, from 1 to (:) 11.
    + Note: we could have written `into = c("use_1", "use_2"...)` but that would have taken a LONG time. Using `paste0()` is much faster.
    
Working with check-all data can be tricky because it requires a lot of columns. We really need to do more transformation of the data. We will revisit this variable in the section on pivoting data.

## Descriptives

So far, we have looked at functions to explore the data and do basic data manipulation and cleaning. We also need to learn functions that allow us to describe specific elements of the data frame.

### Mean - mean()

This is a simple function to allows you to find the mean of data. You can use it via base `R`. Before doing so, note that there is `NA` data, which cannot be calculated in the mean, so we need to use `na.rm = TRUE` to remove the NAs.

```{r} 
#base R
mean(rsurvey$age, na.rm = T)
```

You should note that for both, what was returned was unexpected. For the base `R` sytnax, "NA" was returned. For the `tidyverse` syntax, an error was returned. Both of these occured because the column has `NA`s which cannot be calculated in the `mean()` function. We need to remove the `NA`s:

**A Note About TRUE and FALSE**

>There are many functions that require a logical argument. To save time, you can usually write T or F instead of the whole word.

### Median - median()

You can do the same calculation with the `median()` function:

```{r}
median(rsurvey$age, na.rm = T)
```

### Using the summarize() function - summarize()

We can use the `tidyverse`'s `summarize()` function to do multiple descriptives in one command. `summarize()` is similar to `mutate()` in that it create new columns. However, instead of appending a column to a data frame, it literally summarizes and condenses all data into the number of columns specififed. Here is an example:

```{r}
rsurvey %>%
  drop_na(age) %>%
  summarize(avg = mean(age),
            mid = median(age),
            n = n())
```

What did we do?

  * `rsurvey %>%` - use "rsurvey" and then
    + Note: We are not assigning this to anything because we are just exploring the data. There is no need to create a data object.
  * `drop_na(age) %>% ` - rather than write `na.rm=T` for each function, we will just drop all `NA`s for "age" first and then
  * `summarize(` - create a summary data frame
  * `avg = mean(age)` - create a column "avg" with the mean of "age"
  * `mid = median(age)` - create a column "mid" with the median of "age"
  * `n = n()` - create a column "n" with the sample size of "age"
  
### Analyzing Data by Groups - group_by()

What if we want to view the proportion of ages in the data set, or the ages broken down by group? We can use the `group_by()` function:

```{r}
rsurvey %>%
  drop_na(age) %>%
  group_by(age) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(
    proportion = prop.table(n)
  )
```

Here is what we did:

  * `rsurvey %>%` - use "rsurvey" and then
  * `drop_na(age) %>%` - drop the `NA`s based on "age" and then
  * `group_by(age) %>%` - group the data by "age" and then
  * `summarize(n = n()) %>%` - create a summary data frame that counts the sample size for each group and then
  * `ungroup() %>%` - ungroup the data and then
  * `mutate(proportion = prop.table(n))` - create a new column, "proportion" that takes the proportion of each age based on the count of "n"
  
## Spotting Outliers

In this section, we will look at different ways to identify outliers in univariate data (multivariate data will be covered in [Regression Diagnostics]). Spotting outliers is an important part of the data cleaning process. Many statistical tests rely on the assumption of no extreme scores that may bias the tests. In addition, identifying outliers can help you find mistakes or other problems in the data.

### Detecting Numerical Outliers

We can use a simple scatterplot for numerical data.

```{r}
plot(rsurvey$age)
```

We could also use a boxplot:

```{r}
boxplot(rsurvey$age)
```

The graphs above shows that there are four possible outliers. These outliers have ages over 100. We can search for them in the data to see what is going on with them:

```{r}
rsurvey %>%
  filter(age > 100) %>%
  select(age, qyear_born)
```

It's more than likely these are not serious responses, so we would need to deal with them in some way (see [Dealing with Outliers]).


#### Detecting Categorical Outliers 

We can also see if a category contains any numeric outliers based on another variable. For example, if we want to see if there are any age outliers for the experience variable, we can use a simple boxplot:

```{r}
plot(age ~ qr_experience, data=rsurvey)
```
The graph above shows the four same extreme outliers. It also shows beginner, intermediate, and expert responses have some higher ages, but they don't suggest any surprising outliers, especially because each of the three have high ages.

What did we do?

  * `plot(` - make a plot
  * `age ~ qr_experience,` - plot age by (~) experience
  * `data=rsurvey)` - from the "rsurvey" data frame



### Detecting outliers with Z-scores

One way to detect outliers is to standardize values and select values greater than or less than some specific value. For example, Tabachnick and Fidell (2013) suggest any score above 3.29 may be an outlier for continuous, univariate data.

There are many ways to create Z-scores in R. You could do it manually:

```{r}
rsurvey$z_of_age <- (rsurvey$age - mean(rsurvey$age, na.rm=T))/sd(rsurvey$age, na.rm = T)

# to check
summary(rsurvey$z_of_age)
```

But, we could also load a package and use a function to do it quickly. For example, the `effectsize` package has a `standardize()` function.

```{r message=FALSE, warning=FALSE}
# install package if needed
# install.packages("effectsize")

rsurvey$z_of_age <- effectsize::standardize(rsurvey$age)

# to check
summary(rsurvey$z_of_age)
```

### Dealing with Outliers

#### Removal

One thing you can do is "delete" your outliers. We will set outliers to `NA`.


```{r}
# set ages over 100 to NA
rsurvey <- rsurvey %>%
  mutate(age2 = ifelse(age > 99, NA, age))

# check if we made the correct change
rsurvey %>%
  filter(qyear_born < 1901) %>%
  select(age2, qyear_born)

```
Let's look at the plot again:

```{r}
plot(rsurvey$age2)
```

There are no clear outliers for the "age" variable.

#### Filtering

Rather than transforming that data, you could simply use `filter()` to temporarily remove it for some function. For example:

```{r}
rsurvey %>%
  filter(age < 100) %>%
  with(plot(age))
```

In the formula above, `with()` is a base `R` function that uses the data from a `tidyverse` chain (functions with %>%). Using strictly base R would require more steps, including creating a new dataframe:

```{r}
filtered <- subset(rsurvey, age < 99)
plot(filtered$age)
```

**A Note about "Deleting" Values**

>It is never a good idea to delete or overwrite your raw data. Recall that we have a "raw_data" data object that we have not transformed in any way. All transformations have been done in new data objects and, sometimes, in new variables. This is good practice so that we can assess our changes and revert our changes if necessary.

#### Winsorizing

Winsorizing is a way to replace the most extreme scores with another value. The `psych` package has a `winsor()` function that will replace the top and bottom values with the next highest score.

For example, the code below trims the top and bottom 2% of the data set and replaces it with the next highest and lowest values.

```{r message=FALSE, warning=FALSE}
library(psych)

winsored <- rsurvey %>%
  mutate(winsor = winsor(age, trim=.02, na.rm=T))

# let's check
winsored %>%
  select(age, winsor) %>%
  filter(age < 20 | age > 70)
```


## Assessing Normality

### Histograms

Normality can be visually assessed with a histogram. A quick histogram can be made using base `R`'s `hist()` function:

```{r}
hist(rsurvey$age)
```

More advanced histograms can be made with ggplot. See [Data Visualization with ggplot]

### Density (Curve) Plots

You can also use a density plot to visually assess normality:

```{r}
plot(density(rsurvey$age, na.rm=T))
```

### QQ Plots

A QQ plot can also be used to assess normality:

```{r}
qqnorm(rsurvey$age)
```

### Skewness and Kurtosis

There are a number of ways to get skewness and kurtosis.

You can get the values alongside other stats with the `psych` package's `describe()`

You can also use `skewness()` and `kurtosis()` functions from the `moments` package:

```{r}
# install if needed
# install.packages("moments")

library(moments)

skewness(rsurvey$age, na.rm = T)
kurtosis(rsurvey$age, na.rm = T)
```

### Tests of Normality

Univariate tests of normality are also easily done in `R`. The `stats` package (included with `R`) contains both the Kolmogorov-Smirnov (K-S) test and the Shapiro-Wilk's test. The Shapiro-Wilk's test is more common and easier to compute in `R`:

```{r}
shapiro.test(rsurvey$age)
```

The above p-value is significant at less than 0.05, signifying non-normality.

See `?shapio.test` and `?ks.test` for more information.

### Transforming Variables

If you wish to deal with non-normality by transforming variables, you can use these methods ([DataNovia, n.d.](https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/#transformation-methods)):

  * Square Roots
    + `sqrt(x)` for positive skew
    + `sqrt(max(x+1)-x)` for negative skew
  * Log
    + `log10(x)` for positive skew
    + `log10(max(x+1)-x)` for negative skew
  * Inverse
    + `1/x` for positive skew
    + `1/max(x+1)` for negative skew


## Working with Missing Data

Missing data in `R` is represented by `NA`.
https://towardsdatascience.com/data-cleaning-with-r-and-the-tidyverse-detecting-missing-values-ea23c519bc62

## Identifying missing data

There are a number of ways to identify missing data in R.

### Per Variable

We have already seen a number of ways to view and summarize data [Viewing Data]. There are several other functions that can be useful.

You can use `is.na` to look at `NA` values:


```{r message=FALSE, warning=FALSE}
mean(is.na(rsurvey$age))
```


This means that around 6% of the "age" variable is misisng.

Conversely, you can also use `complete.cases` to see how many cases are *not* missing:

```{r}
  mean(complete.cases(rsurvey$age))
```

94% of the age variable is *not* missing.


### Entire Data Frame

We can look at an entire data frame by using `summarize_all`, part of the `summarize` family of verbs in `dplyr` (part of `tidyverse`).

```{r}
missing <- rsurvey %>%
  summarize_all(funs(mean(is.na(.))))
```

This will produce a new data object where each column has 1 row, and that row is the percent missing. 

What we did:

  * `missing <- ` - assign everything to a new data frame (this step is optional and you could just explore everything in the console)
  * `rsurvey %>% ` - use "rsurvey" and
  * `summarize_all(` - summarize every column
  * `funs(` - use the following function ("funs") to summarize everything
  * `mean(is.na(.))))` - take the mean of missing data.
    + the `.` means to use the entire data frame
    + We need to include it because `is.na` requires an argument
    
**A Note on `summarize`**

> `summarize` is a family of verbs in `dplyr` that includes `summarize`, `summarize_if`, `summarize_at` (for specific columns), and `summarize_all`. You may also see it spelled as `summarise` - this is the same function. Check out `?summarize_all`.

Let's quickly view what that looks like using `glimpse()`. This will print the columns and first row veritcally in the console. Because there are 67 columns, we will just look at the first 10.

```{r}
glimpse(missing[1:10])
```

You can also get a quick visual by running this code:

```{r}
missing %>%
  pivot_longer(qtime:age2, names_to = "variable", 
               values_to = "pct") %>%
  filter(pct > 0) %>%
  filter(!str_detect(variable, "use_")) %>%
  ggplot(aes(x=reorder(variable, pct), y=pct)) +
  coord_flip() +
  geom_col()
```

This code introduces some new concepts that will be covered later, so the following explanation will not go into detail, but here is what we did:

  * `missing %>%` - use the "missing" data frame
  * `pivot_longer(qtime:age2, names_to = "variable", values_to = "pct") %>%` - our "missing" data frame had 1 row and 67 columns. By using `pivot_longer`, we "flipped" our data frame so it had 2 columns ("variable" and "pct") and 67 rows. This is easier to visualize.
  * `filter(pct > 0) %>%` - we removed any columns with no missing data
  * `filter(!str_detect(variable, "use_")) %>%` we removed any columns that started with "use_" because those are check-all questions.
    + note the `!` at the beginning means "is not" or "does not", so this says to filter any columns that does not start with "use_to"
  * `ggplot(aes(x=reorder(variable, pct), y=pct)) +` - this line introduces a powerful visualization tool, `ggplot`, which will be covered later. It says to use "variable on the x-axis (and order it by "pct" so that it is in descending order) and use "pct" on the y-axis.
    + Note that a quirk of `ggplot` is that it uses `+` instead of `%>%` despite being part of the `tidyverse`
  *  `coord_flip() +` this will flip the visual to make the bar chart horizontal
  * `geom_col()` - this means we will make a column (bar) chart


## Dropping missing data

You can remove missing data by using `drop_na()`.

If you use `drop_na()` without including columns in the `()`, it will drop rows that are missing values in any column. This will severely reduce your data frame. If we run the following code, it will drop everything and you will have 0 observations because every row has across has at least 1 missing value somewhere.

```{r}
missing_dropped_all <- rsurvey %>%
  drop_na()

head(missing_dropped_all[1:5])
```

This is useful if you have selected (with `select()` or `subset()`) a single column or a small group of columns and you do want to drop any missing data, but in general, you may want to specify columns:

```{r}
missing_dropped_some <- rsurvey %>%
  drop_na(age)
```

This will remove all rows with misisng data in the age variable.
Compared "rsurvey" to "missing_dropped_some" and you will see less observations.

## Replacing missing data with 0s

Sometimes missing data is meaningful and it would be useful in including them in various analyses. `NA` values are typically dropped from analyses, but we can include them by changing them to 0s. To do so, we can use `mutate()`:

```{r}
zero_ages <- rsurvey %>%
  mutate(age = ifelse(is.na(age), 0, age))

```


Let's compare the mean age of our new data frame and our old one:

```{r}
mean(rsurvey$age, na.rm=T) #note: you must remove NAs for this to run
```

```{r}
mean(zero_ages$age)
```


## Mean imputation

**If warranted**, we can easily replace missing data with means (or medians) like so:

```{r}
mean_imputation <- rsurvey %>%
  mutate(age = ifelse(is.na(age), mean(age, na.rm=T), age))

mean(mean_imputation$age)
```

## Multiple imputation

Multiple imputation is currently beyond the scope of this guide. However, you can learn more at any of the following sites:

  * [Getting Started with Multiple Imputation in R](https://uvastatlab.github.io/2019/05/01/getting-started-with-multiple-imputation-in-r/)
  * [Tutorial on 5 Powerful R Packages used for imputing missing values](https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/)
  * [How do I perform multiple imputation in R?](https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/)
  * [Imputing missing data with R; MICE package](https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/)
  